{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014d5d22-0743-4cb8-8336-f8dd5d81b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import timm\n",
    "from timm.utils import AverageMeter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "from warmup_scheduler import GradualWarmupScheduler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4311ec-628c-48a6-b4e0-ce3e64ee273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd().parent\n",
    "DATA_PATH = ROOT_PATH / \"data\"\n",
    "sys.path.append(ROOT_PATH.as_posix())\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "NUM_EPOCHS = 10\n",
    "SMOOTHING = 0.2\n",
    "\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c88e4dd-2dd4-4357-bb4e-545730620fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import LeafDataset, get_image_labels\n",
    "from src.augmentation import get_augmentations\n",
    "from src.config import AugmentationConfig, ModelConfig\n",
    "from src.utils import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13edb2a7-76fb-4eda-bacd-28d4723aae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_config = AugmentationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55d2f46-fd2c-4561-8faa-c6a10093082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippmoehl/anaconda3/envs/plants/lib/python3.10/site-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n",
      "/Users/philippmoehl/anaconda3/envs/plants/lib/python3.10/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_augs, test_augs = get_augmentations(**aug_config.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6876daf0-7bc3-4506-bd2e-b20b3de2cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15920570-c6df-4049-b580-cd96b7e37e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(backbone, pretrained, optimizer_class, lr, scheduler_class, lr_min, warm_up):\n",
    "    model = timm.create_model(model_name = backbone, pretrained = pretrained)\n",
    "    if optimizer_class == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimzer = None\n",
    "\n",
    "    if scheduler_class == \"CosineAnnealing\":\n",
    "        after_scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer = optimizer, \n",
    "            T_0 = NUM_EPOCHS - warm_up if NUM_EPOCHS > 1 else 1,\n",
    "            eta_min   = lr_min)\n",
    "        scheduler = GradualWarmupScheduler(\n",
    "            optimizer = optimizer, multiplier = 1, total_epoch = warm_up + 1, \n",
    "            after_scheduler = after_scheduler)\n",
    "    else:\n",
    "        scheduler = GradualWarmupScheduler(\n",
    "            optimizer = optimizer, multiplier = 1, total_epoch = warm_up + 1)\n",
    "        \n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa27a29-bd95-4549-b067-6a98044a807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "\n",
    "    def __init__(self, smoothing = 0.1, reduction = 'mean'):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        assert smoothing < 1.0\n",
    "        self.smoothing  = smoothing\n",
    "        self.confidence = 1. - smoothing\n",
    "        self.reduction  = reduction\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae1c5c8-7319-44b8-8c3c-7be9e5f46966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_criterion = LabelSmoothingCrossEntropy(smoothing = SMOOTHING)\n",
    "val_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e0864-dc0e-414e-9e01-104c09396880",
   "metadata": {},
   "source": [
    "CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec730fbf-7807-48d9-8459-1ac052a99b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, labels_all = get_image_labels(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd9df8b-855c-4e55-b2fa-3442d0ee43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646a556d-5d41-443a-ac3b-2e15c9029d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippmoehl/anaconda3/envs/plants/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnet_b5_ns to current tf_efficientnet_b5.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "/Users/philippmoehl/anaconda3/envs/plants/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/Users/philippmoehl/anaconda3/envs/plants/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:152: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, val_index) in enumerate(skf.split(image_paths, labels_all)):\n",
    "    train_dataset = LeafDataset(\n",
    "        [image_paths[i] for i in train_index],\n",
    "        [labels_all[i] for i in train_index],\n",
    "        transform=train_augs\n",
    "    )\n",
    "    val_dataset = LeafDataset(\n",
    "        [image_paths[i] for i in val_index],\n",
    "        [labels_all[i] for i in val_index],\n",
    "        transform=test_augs\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # reset seed\n",
    "    seed_everything(SEED + fold)\n",
    "\n",
    "    model, optimizer, scheduler = get_model(**model_config.model_dump())\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss = AverageMeter()\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            loss.backward() \n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step(epoch + 1 + batch_idx / len(train_loader))  # batch-wise\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = AverageMeter()\n",
    "        _probs = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "    \n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "    \n",
    "                logits = torch.zeros((inputs.shape[0], CFG['num_classes']), device = device)\n",
    "                probs  = torch.zeros((inputs.shape[0], CFG['num_classes']), device = device)\n",
    "    \n",
    "                # compute predictions\n",
    "                logits = model(inputs)\n",
    "                probs = logits.softmax(axis=1)\n",
    "    \n",
    "                # compute loss\n",
    "                loss = criterion(logits, labels)\n",
    "                val_loss.update(loss.item(), inputs.size(0))\n",
    "    \n",
    "                # store predictions\n",
    "                _probs.append(probs.detach().cpu())\n",
    "    \n",
    "        _probs = torch.cat(_probs).numpy()\n",
    "\n",
    "        print(f\"Training loss: {train_loss.sum / len(train_loader)}\")\n",
    "        print(f\"Validation loss: {val_loss.sum / len(val_loader)}\")\n",
    "        \n",
    "        lr = scheduler.state_dict()['_last_lr'][0]\n",
    "\n",
    "        lrs.append(lr)\n",
    "        train_losses.append(train_loss.sum / len(train_dataset))\n",
    "        val_losses.append(val_loss.sum / len(val_dataset))\n",
    "        val_metrics.append(\n",
    "            (np.argmax(_probs, axis = 1) == [labels[i] for i in val_index]).sum() / len(val_dataset)\n",
    "        )\n",
    "        print(f\"Validation accuracy: {val_metrics[epoch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655410ff-e22d-4419-a67d-c73587597d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
